# -*- coding: utf-8 -*-
"""Task5_Future_User_Adoption.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17p2CSR7MuZEJw36c1jbzNYMW0P6bZi-x

## **Task 5 - Future User Adoption**

> The goal is to define an "adopted user" as a user who has logged into the product on three separate days in at least one seven-day period, and then identify which factors predict future user adoption.

> First, I have created a dataframe and added the adopted_user column using the timestamp and visited columns in the takehome_user_engagement csv file 

> Then, I have resolved the following issues in the data given:

1.   creation_time and last_session_creation_time are not datetime objects, hence converted them to datetime objcts
2.   Filled the invited_by_user_id column's NaN values with 0

> I have added active_days column by ombing last_session and creation_time data columns. Then I have merged the features dataframe and the dataframe containing target variable (adopted_user)

> Then I have plotted the scatter matrix for the dataframe. As i couldn't get much information from the scatter matrix, I have plotted the correlation heat map for the data. From the correlation heatmap, I have selected the features which are not highly correlated, dropped the visited column and created a new dataframe for machine learning. 

> Then in order to identify the factors which predict future user adoption, i have made adopted_user as my target(y) variable and rest of the features as input variables(x) and ran randomForrestClassifier algorithm to identify which features are important.

> **Conclusion:**

>From the results of RandomForrestClassifier algorithm, we can see that the important features for predicting the future user adoption are:

*   number of 'active days'('active_days')
*   organization ID ('org_id')
*   invited by user ID ('invited_by_user_id')

> Future research can involve using other useful algorithms for Feature Selection like Recursive Feature Elimination (RFE) using Logistic Regression as an estimator, Embedded methods like Lasso etc.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import datetime  
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(101)

# %matplotlib inline

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

users = pd.read_csv('/content/takehome_users.csv',encoding='ISO-8859-1')
users.head()

users.info()

users.rename(columns={'object_id':'user_id'}, inplace=True)

users_eng = pd.read_csv('/content/takehome_user_engagement.csv')
users_eng.head()

users_eng.info()

users_eng.time_stamp = pd.to_datetime(users_eng.time_stamp)

users_eng1 = users_eng.set_index('time_stamp').groupby('user_id')['visited'].resample('D').count()

users_eng1=pd.DataFrame(users_eng1)
users_eng1.head()

users_eng1 = users_eng1.rolling(window=7, min_periods=1).sum()

users_eng1.reset_index(inplace=True)

users_eng1.head()

users_eng2 = users_eng1.groupby('user_id')[['visited']].max()

users_eng2.head()

users_eng2.loc[users_eng2['visited'] >= 3, 'adopted_user'] = 1
users_eng2.loc[users_eng2['visited'] < 3, 'adopted_user'] = 0

users_eng2['visited'] = users_eng2['visited'].astype(int)
users_eng2['adopted_user'] = users_eng2['adopted_user'].astype(int)

users_eng2.reset_index(inplace=True)

users_eng2.head()

df = users_eng2.merge(users, how='inner', on='user_id', sort=True)

df.head()

sns.countplot(x=df['adopted_user'])

df.isnull().sum()

df['invited_by_user_id'].fillna(0, inplace=True)
df['invited_by_user_id'] = df['invited_by_user_id'].astype(int)

df.creation_time = pd.to_datetime(df.creation_time)
df.last_session_creation_time = pd.to_datetime(df.last_session_creation_time)
df['active_days'] = df['creation_time'] - df['last_session_creation_time']
df['active_days'] = df['active_days'].dt.days

df.head()

last_column = df['adopted_user']
df.drop(labels=['adopted_user'], axis=1, inplace = True)
df.insert(12, 'adopted_user', last_column)
df

import matplotlib.pyplot as plt
from pandas.plotting import scatter_matrix

scatter_matrix(df)
plt.show()

import matplotlib.pyplot as plt #seaborn
sns.heatmap(df.corr(),annot=True)
fig = plt.gcf()
fig.set_size_inches(8,8)

#selecting features for machine learning

df1 = df[['creation_source','opted_in_to_mailing_list', 'enabled_for_marketing_drip', 
              'org_id', 'invited_by_user_id', 'active_days', 'adopted_user' ]]

#creating dummies for 'creation_source' variable
creation_source_dum = pd.get_dummies(df1['creation_source'], drop_first=True)
df1.drop(['creation_source'], axis=1, inplace=True)

df_ml = pd.concat([df1, creation_source_dum], axis=1)

df_ml.head()

last_column = df_ml['adopted_user']
df_ml.drop(labels=['adopted_user'], axis=1, inplace = True)
df_ml.insert(9, 'adopted_user', last_column)
df_ml

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()
x=df_ml[df_ml.columns[:9]]
y=df1.adopted_user
clf.fit(x,y)
feature_imp = pd.DataFrame(clf.feature_importances_,index=x.columns)
feature_imp.sort_values(by = 0 , ascending = False)